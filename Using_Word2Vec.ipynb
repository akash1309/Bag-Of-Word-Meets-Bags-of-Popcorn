{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using Word2Vec.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1tEUVgCSYtAyXfXCPNeksGssbe8ol8GUf",
      "authorship_tag": "ABX9TyPpKjXCx+QjZqRWgT3lBorR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxWdi_VHYvqc",
        "colab_type": "text"
      },
      "source": [
        "# Using Word2Vec Algorithm\n",
        "\n",
        "It works on unlabelled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhSMERMZLWvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import gensim\n",
        "import cython"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61AMoQhtD8hR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c7c2505f-bae2-4593-89f5-75c06aa4bbe5"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvDPFOBLZIen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgvby0zSZPkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lab_df = pd.read_csv('drive/My Drive/Pytorch_DataSet/Bag Of Words Meets Bags of popcorn/labeledTrainData.tsv',sep='\\t')\n",
        "unlab_df = pd.read_csv('drive/My Drive/Pytorch_DataSet/Bag Of Words Meets Bags of popcorn/unlabeledTrainData.tsv',delimiter=\"\\t\", quoting=3)\n",
        "test_df = pd.read_csv('drive/My Drive/Pytorch_DataSet/Bag Of Words Meets Bags of popcorn/testData.tsv',sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiOl7XmDaAtk",
        "colab_type": "code",
        "outputId": "37118d37-f4c9-4202-8db5-715b6be99047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "lab_df.head(2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5814_8</td>\n",
              "      <td>1</td>\n",
              "      <td>With all this stuff going down at the moment w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2381_9</td>\n",
              "      <td>1</td>\n",
              "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  sentiment                                             review\n",
              "0  5814_8          1  With all this stuff going down at the moment w...\n",
              "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrUAgO9RaEBS",
        "colab_type": "code",
        "outputId": "dcbad20a-503c-442e-edfd-cafb526f7276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "unlab_df.head(2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"9999_0\"</td>\n",
              "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"45057_0\"</td>\n",
              "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id                                             review\n",
              "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
              "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkVQ3yg_aGxW",
        "colab_type": "code",
        "outputId": "c2a64d97-fc54-4806-fc1b-f2c1cc5dcf53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(lab_df), len(unlab_df), len(test_df)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 50000, 25000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmBVjTfyDbyC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "adaa4dbd-7389-4676-f048-a1394c7001f3"
      },
      "source": [
        "from nltk.corpus import stopwords # Import the stop word list\n",
        "print(stopwords.words(\"english\"))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEAYvbzpcJuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleaning text\n",
        "\"\"\"\n",
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});','',text)  # for removal of html tags\n",
        "  text = re.sub(r\"[^a-zA-Z]\",\" \",text)\n",
        "  text = re.sub('\\W', ' ', text)  # If the comment/word does not contain any alphabets\n",
        "  text = text.strip(' ') # Removing leading and trailing white spaces\n",
        "  text = text.split()  # For words splitting\n",
        "  return text\n",
        "\"\"\"\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def review_to_wordlist(review, remove_stopwords=False ):\n",
        "    \n",
        "    # 1. Removing html tags\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "    #review_text = re.sub('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});','',review)  \n",
        "\n",
        "    # 2. Remove non-letters\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    \n",
        "    # 3. Convert words to lower case and split them\n",
        "    words = review_text.lower().split()\n",
        "    #\n",
        "    # 4. Optionally remove stop words (false by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    #\n",
        "    # 5. Return a list of words\n",
        "    return(words)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE4D5uZKd7ZF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "edba4719-96b0-4fc6-91c7-71b7f61f6bfd"
      },
      "source": [
        "\"\"\"\n",
        "lab_df['review'] = lab_df['review'].apply(lambda text: clean_text(text))\n",
        "unlab_df['review'] = unlab_df['review'].apply(lambda text: clean_text(text))\n",
        "test_df['review'] = test_df['review'].apply(lambda text: clean_text(text))\n",
        "\"\"\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nlab_df['review'] = lab_df['review'].apply(lambda text: clean_text(text))\\nunlab_df['review'] = unlab_df['review'].apply(lambda text: clean_text(text))\\ntest_df['review'] = test_df['review'].apply(lambda text: clean_text(text))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnlC9qKifgBc",
        "colab_type": "text"
      },
      "source": [
        "Next, we want a specific input format. Word2Vec expects single sentences, each one as a list of words. In other words, the input format is a list of lists.\n",
        "\n",
        "It is not at all straightforward how to split a paragraph into sentences. There are all kinds of gotchas in natural language. English sentences can end with \"?\", \"!\", \"\"\", or \".\", among other things, and spacing and capitalization are not reliable guides either. For this reason, we'll use NLTK's punkt tokenizer for sentence splitting. In order to use this, you will need to install NLTK and use nltk.download() to download the relevant training file for punkt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IsOAQXDeOzR",
        "colab_type": "code",
        "outputId": "1637862e-af7a-45f2-a87e-bd21ed77dbdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk.data\n",
        "nltk.download('punkt')   \n",
        "\n",
        "# Load the punkt tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhUJVXS_fqrQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9de4c57b-09f3-4847-862e-cfa1da89bec6"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<nltk.tokenize.punkt.PunktSentenceTokenizer at 0x7fbf215caeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAASNwYxCWPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function to split a review into parsed sentences\n",
        "\n",
        "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
        "\n",
        "    # Function to split a review into parsed sentences. Returns a list of sentences, where each sentence is a list of words\n",
        "    \n",
        "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
        "\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    \n",
        "    # 2. Loop over each sentence\n",
        "\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        # If a sentence is empty, skip it\n",
        "        if len(raw_sentence) > 0:\n",
        "            # Otherwise, call review_to_wordlist to get a list of words\n",
        "            sentences.append(review_to_wordlist( raw_sentence, remove_stopwords ))\n",
        "    \n",
        "    # Return the list of sentences (each sentence is a list of words,\n",
        "    # so this returns a list of lists\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XP0-KgtDnCY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "483737a4-11e2-4066-9a07-4dbf07a882aa"
      },
      "source": [
        "sentences = []  # Initialize an empty list of sentences\n",
        "\n",
        "print(\"Parsing sentences from training set\")\n",
        "for review in lab_df[\"review\"]:\n",
        "    #print(type(review))\n",
        "    sentences += review_to_sentences(review, tokenizer)\n",
        "\n",
        "print(\"Parsing sentences from unlabeled set\")\n",
        "for review in unlab_df[\"review\"]:\n",
        "    sentences += review_to_sentences(review, tokenizer)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing sentences from training set\n",
            "Parsing sentences from unlabeled set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOYQ3eZ9FZlm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e574adca-11be-4b5a-91d8-6e31a07f776a"
      },
      "source": [
        "print(len(sentences))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "795872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mIOHU6pIK9Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "577fd123-28cf-4b00-d892-6af5148e3068"
      },
      "source": [
        "' '.join(sentences[0])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'with all this stuff going down at the moment with mj i ve started listening to his music watching the odd documentary here and there watched the wiz and watched moonwalker again'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-o7TcfCJ728",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0898bc55-7c1b-46f4-c1d6-cb779d894879"
      },
      "source": [
        "# Import the built-in logging module and configure it so that Word2Vec \n",
        "# creates nice output messages\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
        "    level=logging.INFO)\n",
        "\n",
        "# Set values for various parameters\n",
        "num_features = 300    # Word vector dimensionality                      \n",
        "min_word_count = 40   # Minimum word count                        \n",
        "num_workers = 4       # Number of threads to run in parallel\n",
        "context = 10          # Context window size                                                                                    \n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "# Initialize and train the model (this will take some time)\n",
        "from gensim.models import word2vec\n",
        "print(\"Training model...\")\n",
        "model = word2vec.Word2Vec(sentences, workers=num_workers, \n",
        "            size=num_features, min_count = min_word_count,\n",
        "            window = context, sample = downsampling)\n",
        "\n",
        "# If you don't plan to train the model any further, calling \n",
        "# init_sims will make the model much more memory-efficient.\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "# It can be helpful to create a meaningful model name and \n",
        "# save the model for later use. You can load it later using Word2Vec.load()\n",
        "model_name = \"300features_40minwords_10context\"\n",
        "model.save(model_name)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-24 15:33:20,179 : INFO : collecting all words and their counts\n",
            "2020-05-24 15:33:20,184 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2020-05-24 15:33:20,244 : INFO : PROGRESS: at sentence #10000, processed 225664 words, keeping 17775 word types\n",
            "2020-05-24 15:33:20,298 : INFO : PROGRESS: at sentence #20000, processed 451582 words, keeping 24944 word types\n",
            "2020-05-24 15:33:20,349 : INFO : PROGRESS: at sentence #30000, processed 670632 words, keeping 30023 word types\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-24 15:33:20,406 : INFO : PROGRESS: at sentence #40000, processed 896478 words, keeping 34329 word types\n",
            "2020-05-24 15:33:20,458 : INFO : PROGRESS: at sentence #50000, processed 1115469 words, keeping 37741 word types\n",
            "2020-05-24 15:33:20,509 : INFO : PROGRESS: at sentence #60000, processed 1336692 words, keeping 40702 word types\n",
            "2020-05-24 15:33:20,557 : INFO : PROGRESS: at sentence #70000, processed 1559365 words, keeping 43300 word types\n",
            "2020-05-24 15:33:20,612 : INFO : PROGRESS: at sentence #80000, processed 1778623 words, keeping 45699 word types\n",
            "2020-05-24 15:33:20,658 : INFO : PROGRESS: at sentence #90000, processed 2002603 words, keeping 48113 word types\n",
            "2020-05-24 15:33:20,711 : INFO : PROGRESS: at sentence #100000, processed 2224101 words, keeping 50180 word types\n",
            "2020-05-24 15:33:20,756 : INFO : PROGRESS: at sentence #110000, processed 2442894 words, keeping 52050 word types\n",
            "2020-05-24 15:33:20,806 : INFO : PROGRESS: at sentence #120000, processed 2665092 words, keeping 54089 word types\n",
            "2020-05-24 15:33:20,857 : INFO : PROGRESS: at sentence #130000, processed 2890948 words, keeping 55829 word types\n",
            "2020-05-24 15:33:20,907 : INFO : PROGRESS: at sentence #140000, processed 3103390 words, keeping 57318 word types\n",
            "2020-05-24 15:33:20,958 : INFO : PROGRESS: at sentence #150000, processed 3328823 words, keeping 59031 word types\n",
            "2020-05-24 15:33:21,011 : INFO : PROGRESS: at sentence #160000, processed 3550557 words, keeping 60569 word types\n",
            "2020-05-24 15:33:21,066 : INFO : PROGRESS: at sentence #170000, processed 3773286 words, keeping 62032 word types\n",
            "2020-05-24 15:33:21,118 : INFO : PROGRESS: at sentence #180000, processed 3994239 words, keeping 63473 word types\n",
            "2020-05-24 15:33:21,167 : INFO : PROGRESS: at sentence #190000, processed 4219119 words, keeping 64766 word types\n",
            "2020-05-24 15:33:21,217 : INFO : PROGRESS: at sentence #200000, processed 4443600 words, keeping 66056 word types\n",
            "2020-05-24 15:33:21,274 : INFO : PROGRESS: at sentence #210000, processed 4663901 words, keeping 67353 word types\n",
            "2020-05-24 15:33:21,328 : INFO : PROGRESS: at sentence #220000, processed 4888988 words, keeping 68632 word types\n",
            "2020-05-24 15:33:21,378 : INFO : PROGRESS: at sentence #230000, processed 5111142 words, keeping 69909 word types\n",
            "2020-05-24 15:33:21,433 : INFO : PROGRESS: at sentence #240000, processed 5338157 words, keeping 71133 word types\n",
            "2020-05-24 15:33:21,481 : INFO : PROGRESS: at sentence #250000, processed 5552543 words, keeping 72322 word types\n",
            "2020-05-24 15:33:21,531 : INFO : PROGRESS: at sentence #260000, processed 5772532 words, keeping 73454 word types\n",
            "2020-05-24 15:33:21,589 : INFO : PROGRESS: at sentence #270000, processed 5992302 words, keeping 74713 word types\n",
            "2020-05-24 15:33:21,638 : INFO : PROGRESS: at sentence #280000, processed 6218589 words, keeping 76300 word types\n",
            "2020-05-24 15:33:21,685 : INFO : PROGRESS: at sentence #290000, processed 6441501 words, keeping 77782 word types\n",
            "2020-05-24 15:33:21,735 : INFO : PROGRESS: at sentence #300000, processed 6666056 words, keeping 79128 word types\n",
            "2020-05-24 15:33:21,783 : INFO : PROGRESS: at sentence #310000, processed 6890439 words, keeping 80423 word types\n",
            "2020-05-24 15:33:21,834 : INFO : PROGRESS: at sentence #320000, processed 7117442 words, keeping 81782 word types\n",
            "2020-05-24 15:33:21,884 : INFO : PROGRESS: at sentence #330000, processed 7339075 words, keeping 82995 word types\n",
            "2020-05-24 15:33:21,937 : INFO : PROGRESS: at sentence #340000, processed 7568395 words, keeping 84249 word types\n",
            "2020-05-24 15:33:21,989 : INFO : PROGRESS: at sentence #350000, processed 7791752 words, keeping 85404 word types\n",
            "2020-05-24 15:33:22,040 : INFO : PROGRESS: at sentence #360000, processed 8011384 words, keeping 86565 word types\n",
            "2020-05-24 15:33:22,097 : INFO : PROGRESS: at sentence #370000, processed 8239070 words, keeping 87662 word types\n",
            "2020-05-24 15:33:22,150 : INFO : PROGRESS: at sentence #380000, processed 8464887 words, keeping 88840 word types\n",
            "2020-05-24 15:33:22,205 : INFO : PROGRESS: at sentence #390000, processed 8693660 words, keeping 89878 word types\n",
            "2020-05-24 15:33:22,255 : INFO : PROGRESS: at sentence #400000, processed 8917611 words, keeping 90882 word types\n",
            "2020-05-24 15:33:22,306 : INFO : PROGRESS: at sentence #410000, processed 9138559 words, keeping 91859 word types\n",
            "2020-05-24 15:33:22,356 : INFO : PROGRESS: at sentence #420000, processed 9358876 words, keeping 92882 word types\n",
            "2020-05-24 15:33:22,410 : INFO : PROGRESS: at sentence #430000, processed 9587270 words, keeping 93912 word types\n",
            "2020-05-24 15:33:22,478 : INFO : PROGRESS: at sentence #440000, processed 9813090 words, keeping 94858 word types\n",
            "2020-05-24 15:33:22,533 : INFO : PROGRESS: at sentence #450000, processed 10037669 words, keeping 95999 word types\n",
            "2020-05-24 15:33:22,587 : INFO : PROGRESS: at sentence #460000, processed 10270891 words, keeping 97068 word types\n",
            "2020-05-24 15:33:22,638 : INFO : PROGRESS: at sentence #470000, processed 10497516 words, keeping 97892 word types\n",
            "2020-05-24 15:33:22,688 : INFO : PROGRESS: at sentence #480000, processed 10718677 words, keeping 98822 word types\n",
            "2020-05-24 15:33:22,737 : INFO : PROGRESS: at sentence #490000, processed 10945340 words, keeping 99846 word types\n",
            "2020-05-24 15:33:22,785 : INFO : PROGRESS: at sentence #500000, processed 11167018 words, keeping 100743 word types\n",
            "2020-05-24 15:33:22,834 : INFO : PROGRESS: at sentence #510000, processed 11392468 words, keeping 101680 word types\n",
            "2020-05-24 15:33:22,883 : INFO : PROGRESS: at sentence #520000, processed 11615357 words, keeping 102564 word types\n",
            "2020-05-24 15:33:22,931 : INFO : PROGRESS: at sentence #530000, processed 11840724 words, keeping 103382 word types\n",
            "2020-05-24 15:33:22,983 : INFO : PROGRESS: at sentence #540000, processed 12064929 words, keeping 104239 word types\n",
            "2020-05-24 15:33:23,038 : INFO : PROGRESS: at sentence #550000, processed 12290277 words, keeping 105107 word types\n",
            "2020-05-24 15:33:23,087 : INFO : PROGRESS: at sentence #560000, processed 12511689 words, keeping 105982 word types\n",
            "2020-05-24 15:33:23,137 : INFO : PROGRESS: at sentence #570000, processed 12739997 words, keeping 106765 word types\n",
            "2020-05-24 15:33:23,187 : INFO : PROGRESS: at sentence #580000, processed 12961661 words, keeping 107628 word types\n",
            "2020-05-24 15:33:23,240 : INFO : PROGRESS: at sentence #590000, processed 13187373 words, keeping 108479 word types\n",
            "2020-05-24 15:33:23,295 : INFO : PROGRESS: at sentence #600000, processed 13409391 words, keeping 109195 word types\n",
            "2020-05-24 15:33:23,350 : INFO : PROGRESS: at sentence #610000, processed 13631453 words, keeping 110066 word types\n",
            "2020-05-24 15:33:23,403 : INFO : PROGRESS: at sentence #620000, processed 13856992 words, keeping 110810 word types\n",
            "2020-05-24 15:33:23,461 : INFO : PROGRESS: at sentence #630000, processed 14080109 words, keeping 111581 word types\n",
            "2020-05-24 15:33:23,515 : INFO : PROGRESS: at sentence #640000, processed 14301626 words, keeping 112397 word types\n",
            "2020-05-24 15:33:23,571 : INFO : PROGRESS: at sentence #650000, processed 14528096 words, keeping 113175 word types\n",
            "2020-05-24 15:33:23,630 : INFO : PROGRESS: at sentence #660000, processed 14749998 words, keeping 113902 word types\n",
            "2020-05-24 15:33:23,685 : INFO : PROGRESS: at sentence #670000, processed 14974475 words, keeping 114623 word types\n",
            "2020-05-24 15:33:23,737 : INFO : PROGRESS: at sentence #680000, processed 15199010 words, keeping 115339 word types\n",
            "2020-05-24 15:33:23,787 : INFO : PROGRESS: at sentence #690000, processed 15421656 words, keeping 116109 word types\n",
            "2020-05-24 15:33:23,837 : INFO : PROGRESS: at sentence #700000, processed 15650148 words, keeping 116922 word types\n",
            "2020-05-24 15:33:23,884 : INFO : PROGRESS: at sentence #710000, processed 15871849 words, keeping 117568 word types\n",
            "2020-05-24 15:33:23,932 : INFO : PROGRESS: at sentence #720000, processed 16098974 words, keeping 118203 word types\n",
            "2020-05-24 15:33:23,981 : INFO : PROGRESS: at sentence #730000, processed 16323039 words, keeping 118927 word types\n",
            "2020-05-24 15:33:24,028 : INFO : PROGRESS: at sentence #740000, processed 16546073 words, keeping 119639 word types\n",
            "2020-05-24 15:33:24,078 : INFO : PROGRESS: at sentence #750000, processed 16763375 words, keeping 120274 word types\n",
            "2020-05-24 15:33:24,126 : INFO : PROGRESS: at sentence #760000, processed 16982901 words, keeping 120901 word types\n",
            "2020-05-24 15:33:24,174 : INFO : PROGRESS: at sentence #770000, processed 17209327 words, keeping 121680 word types\n",
            "2020-05-24 15:33:24,224 : INFO : PROGRESS: at sentence #780000, processed 17440408 words, keeping 122381 word types\n",
            "2020-05-24 15:33:24,274 : INFO : PROGRESS: at sentence #790000, processed 17666965 words, keeping 123048 word types\n",
            "2020-05-24 15:33:24,304 : INFO : collected 123504 word types from a corpus of 17798082 raw words and 795872 sentences\n",
            "2020-05-24 15:33:24,305 : INFO : Loading a fresh vocabulary\n",
            "2020-05-24 15:33:24,386 : INFO : effective_min_count=40 retains 16490 unique words (13% of original 123504, drops 107014)\n",
            "2020-05-24 15:33:24,387 : INFO : effective_min_count=40 leaves 17238940 word corpus (96% of original 17798082, drops 559142)\n",
            "2020-05-24 15:33:24,440 : INFO : deleting the raw counts dictionary of 123504 items\n",
            "2020-05-24 15:33:24,446 : INFO : sample=0.001 downsamples 48 most-common words\n",
            "2020-05-24 15:33:24,447 : INFO : downsampling leaves estimated 12749658 word corpus (74.0% of prior 17238940)\n",
            "2020-05-24 15:33:24,492 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
            "2020-05-24 15:33:24,493 : INFO : resetting layer weights\n",
            "2020-05-24 15:33:27,464 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
            "2020-05-24 15:33:28,476 : INFO : EPOCH 1 - PROGRESS: at 2.97% examples, 377621 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:29,492 : INFO : EPOCH 1 - PROGRESS: at 5.93% examples, 375126 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:30,539 : INFO : EPOCH 1 - PROGRESS: at 9.12% examples, 377718 words/s, in_qsize 7, out_qsize 1\n",
            "2020-05-24 15:33:31,600 : INFO : EPOCH 1 - PROGRESS: at 12.36% examples, 379524 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:32,614 : INFO : EPOCH 1 - PROGRESS: at 15.52% examples, 383029 words/s, in_qsize 7, out_qsize 1\n",
            "2020-05-24 15:33:33,638 : INFO : EPOCH 1 - PROGRESS: at 18.76% examples, 385103 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:34,648 : INFO : EPOCH 1 - PROGRESS: at 21.88% examples, 385673 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:35,660 : INFO : EPOCH 1 - PROGRESS: at 24.96% examples, 386297 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:36,684 : INFO : EPOCH 1 - PROGRESS: at 28.17% examples, 387342 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:37,687 : INFO : EPOCH 1 - PROGRESS: at 31.35% examples, 388472 words/s, in_qsize 6, out_qsize 2\n",
            "2020-05-24 15:33:38,690 : INFO : EPOCH 1 - PROGRESS: at 34.52% examples, 389357 words/s, in_qsize 8, out_qsize 2\n",
            "2020-05-24 15:33:39,697 : INFO : EPOCH 1 - PROGRESS: at 37.87% examples, 392387 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:40,749 : INFO : EPOCH 1 - PROGRESS: at 41.06% examples, 391994 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:41,758 : INFO : EPOCH 1 - PROGRESS: at 44.17% examples, 392363 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:33:42,760 : INFO : EPOCH 1 - PROGRESS: at 47.30% examples, 392824 words/s, in_qsize 8, out_qsize 0\n",
            "2020-05-24 15:33:43,810 : INFO : EPOCH 1 - PROGRESS: at 50.53% examples, 392990 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:33:44,823 : INFO : EPOCH 1 - PROGRESS: at 53.79% examples, 393974 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:45,829 : INFO : EPOCH 1 - PROGRESS: at 56.84% examples, 393799 words/s, in_qsize 8, out_qsize 1\n",
            "2020-05-24 15:33:46,875 : INFO : EPOCH 1 - PROGRESS: at 60.01% examples, 393909 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:33:47,909 : INFO : EPOCH 1 - PROGRESS: at 63.35% examples, 394598 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:48,926 : INFO : EPOCH 1 - PROGRESS: at 66.46% examples, 394605 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:49,946 : INFO : EPOCH 1 - PROGRESS: at 69.55% examples, 394213 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:50,957 : INFO : EPOCH 1 - PROGRESS: at 72.79% examples, 394901 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:51,971 : INFO : EPOCH 1 - PROGRESS: at 75.89% examples, 394623 words/s, in_qsize 8, out_qsize 1\n",
            "2020-05-24 15:33:52,972 : INFO : EPOCH 1 - PROGRESS: at 78.96% examples, 394562 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:53,995 : INFO : EPOCH 1 - PROGRESS: at 82.22% examples, 394982 words/s, in_qsize 8, out_qsize 0\n",
            "2020-05-24 15:33:55,026 : INFO : EPOCH 1 - PROGRESS: at 85.35% examples, 394738 words/s, in_qsize 8, out_qsize 1\n",
            "2020-05-24 15:33:56,026 : INFO : EPOCH 1 - PROGRESS: at 88.58% examples, 395428 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:33:57,030 : INFO : EPOCH 1 - PROGRESS: at 91.66% examples, 395303 words/s, in_qsize 8, out_qsize 0\n",
            "2020-05-24 15:33:58,056 : INFO : EPOCH 1 - PROGRESS: at 94.77% examples, 394986 words/s, in_qsize 7, out_qsize 3\n",
            "2020-05-24 15:33:59,053 : INFO : EPOCH 1 - PROGRESS: at 97.89% examples, 395115 words/s, in_qsize 8, out_qsize 1\n",
            "2020-05-24 15:33:59,674 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-24 15:33:59,678 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-24 15:33:59,679 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-24 15:33:59,699 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-24 15:33:59,700 : INFO : EPOCH - 1 : training on 17798082 raw words (12749319 effective words) took 32.2s, 395603 effective words/s\n",
            "2020-05-24 15:34:00,753 : INFO : EPOCH 2 - PROGRESS: at 3.03% examples, 369553 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:01,783 : INFO : EPOCH 2 - PROGRESS: at 6.24% examples, 382575 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:34:02,827 : INFO : EPOCH 2 - PROGRESS: at 9.52% examples, 387488 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:34:03,864 : INFO : EPOCH 2 - PROGRESS: at 12.76% examples, 388928 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:34:04,875 : INFO : EPOCH 2 - PROGRESS: at 15.91% examples, 390431 words/s, in_qsize 5, out_qsize 2\n",
            "2020-05-24 15:34:05,938 : INFO : EPOCH 2 - PROGRESS: at 19.28% examples, 391371 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:34:06,950 : INFO : EPOCH 2 - PROGRESS: at 22.50% examples, 392945 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:07,956 : INFO : EPOCH 2 - PROGRESS: at 25.77% examples, 395250 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:08,990 : INFO : EPOCH 2 - PROGRESS: at 28.84% examples, 393455 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:10,025 : INFO : EPOCH 2 - PROGRESS: at 32.10% examples, 393515 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:11,049 : INFO : EPOCH 2 - PROGRESS: at 35.41% examples, 395132 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:12,054 : INFO : EPOCH 2 - PROGRESS: at 38.54% examples, 395425 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:13,098 : INFO : EPOCH 2 - PROGRESS: at 41.62% examples, 394004 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:14,112 : INFO : EPOCH 2 - PROGRESS: at 44.70% examples, 393560 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:34:15,167 : INFO : EPOCH 2 - PROGRESS: at 47.86% examples, 393015 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:16,174 : INFO : EPOCH 2 - PROGRESS: at 51.03% examples, 393803 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:17,176 : INFO : EPOCH 2 - PROGRESS: at 54.12% examples, 393706 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:18,177 : INFO : EPOCH 2 - PROGRESS: at 57.11% examples, 393272 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:19,192 : INFO : EPOCH 2 - PROGRESS: at 60.20% examples, 393337 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:20,236 : INFO : EPOCH 2 - PROGRESS: at 63.34% examples, 392843 words/s, in_qsize 8, out_qsize 2\n",
            "2020-05-24 15:34:21,248 : INFO : EPOCH 2 - PROGRESS: at 66.53% examples, 393332 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:34:22,255 : INFO : EPOCH 2 - PROGRESS: at 69.55% examples, 392914 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:23,267 : INFO : EPOCH 2 - PROGRESS: at 72.68% examples, 393034 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:24,282 : INFO : EPOCH 2 - PROGRESS: at 75.60% examples, 391954 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:25,328 : INFO : EPOCH 2 - PROGRESS: at 78.62% examples, 391013 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:34:26,340 : INFO : EPOCH 2 - PROGRESS: at 81.65% examples, 390636 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:27,355 : INFO : EPOCH 2 - PROGRESS: at 84.57% examples, 389746 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:28,372 : INFO : EPOCH 2 - PROGRESS: at 87.48% examples, 388893 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:34:29,387 : INFO : EPOCH 2 - PROGRESS: at 90.43% examples, 388358 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:30,391 : INFO : EPOCH 2 - PROGRESS: at 93.40% examples, 388003 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:31,413 : INFO : EPOCH 2 - PROGRESS: at 96.36% examples, 387219 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:34:32,433 : INFO : EPOCH 2 - PROGRESS: at 99.22% examples, 386535 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:32,657 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-24 15:34:32,663 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-24 15:34:32,681 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-24 15:34:32,691 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-24 15:34:32,692 : INFO : EPOCH - 2 : training on 17798082 raw words (12748929 effective words) took 33.0s, 386516 effective words/s\n",
            "2020-05-24 15:34:33,739 : INFO : EPOCH 3 - PROGRESS: at 2.80% examples, 344721 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:34,751 : INFO : EPOCH 3 - PROGRESS: at 5.60% examples, 348913 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:35,789 : INFO : EPOCH 3 - PROGRESS: at 8.51% examples, 349699 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:36,790 : INFO : EPOCH 3 - PROGRESS: at 11.50% examples, 356645 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:37,802 : INFO : EPOCH 3 - PROGRESS: at 14.63% examples, 363225 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:38,808 : INFO : EPOCH 3 - PROGRESS: at 17.76% examples, 367789 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:39,866 : INFO : EPOCH 3 - PROGRESS: at 20.96% examples, 370376 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:40,914 : INFO : EPOCH 3 - PROGRESS: at 24.11% examples, 372154 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:41,916 : INFO : EPOCH 3 - PROGRESS: at 27.11% examples, 372550 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:42,983 : INFO : EPOCH 3 - PROGRESS: at 30.06% examples, 370757 words/s, in_qsize 7, out_qsize 1\n",
            "2020-05-24 15:34:43,984 : INFO : EPOCH 3 - PROGRESS: at 33.06% examples, 370769 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:34:44,997 : INFO : EPOCH 3 - PROGRESS: at 35.92% examples, 369882 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:46,031 : INFO : EPOCH 3 - PROGRESS: at 38.65% examples, 367482 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:34:47,067 : INFO : EPOCH 3 - PROGRESS: at 41.45% examples, 365863 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:34:48,085 : INFO : EPOCH 3 - PROGRESS: at 44.35% examples, 365876 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:49,109 : INFO : EPOCH 3 - PROGRESS: at 47.19% examples, 365257 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:34:50,149 : INFO : EPOCH 3 - PROGRESS: at 50.13% examples, 365229 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:51,167 : INFO : EPOCH 3 - PROGRESS: at 53.12% examples, 365618 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:52,199 : INFO : EPOCH 3 - PROGRESS: at 56.07% examples, 365687 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:53,205 : INFO : EPOCH 3 - PROGRESS: at 58.86% examples, 365526 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:54,238 : INFO : EPOCH 3 - PROGRESS: at 61.65% examples, 364588 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:34:55,280 : INFO : EPOCH 3 - PROGRESS: at 64.57% examples, 364267 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:56,283 : INFO : EPOCH 3 - PROGRESS: at 67.36% examples, 363924 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:57,283 : INFO : EPOCH 3 - PROGRESS: at 70.17% examples, 363668 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:58,308 : INFO : EPOCH 3 - PROGRESS: at 73.07% examples, 363652 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:34:59,312 : INFO : EPOCH 3 - PROGRESS: at 75.95% examples, 363648 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:35:00,323 : INFO : EPOCH 3 - PROGRESS: at 78.74% examples, 363278 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:01,340 : INFO : EPOCH 3 - PROGRESS: at 81.65% examples, 363365 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:02,367 : INFO : EPOCH 3 - PROGRESS: at 84.62% examples, 363563 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:03,381 : INFO : EPOCH 3 - PROGRESS: at 87.54% examples, 363664 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:04,390 : INFO : EPOCH 3 - PROGRESS: at 90.25% examples, 363144 words/s, in_qsize 8, out_qsize 1\n",
            "2020-05-24 15:35:05,413 : INFO : EPOCH 3 - PROGRESS: at 93.29% examples, 363590 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:06,445 : INFO : EPOCH 3 - PROGRESS: at 96.25% examples, 363487 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:07,512 : INFO : EPOCH 3 - PROGRESS: at 99.16% examples, 363238 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:07,711 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-24 15:35:07,731 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-24 15:35:07,751 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-24 15:35:07,759 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-24 15:35:07,760 : INFO : EPOCH - 3 : training on 17798082 raw words (12751460 effective words) took 35.1s, 363712 effective words/s\n",
            "2020-05-24 15:35:08,771 : INFO : EPOCH 4 - PROGRESS: at 2.74% examples, 350136 words/s, in_qsize 8, out_qsize 1\n",
            "2020-05-24 15:35:09,775 : INFO : EPOCH 4 - PROGRESS: at 5.60% examples, 356597 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:10,802 : INFO : EPOCH 4 - PROGRESS: at 8.51% examples, 355973 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:11,843 : INFO : EPOCH 4 - PROGRESS: at 11.51% examples, 358140 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:12,889 : INFO : EPOCH 4 - PROGRESS: at 14.51% examples, 359072 words/s, in_qsize 8, out_qsize 3\n",
            "2020-05-24 15:35:13,901 : INFO : EPOCH 4 - PROGRESS: at 17.65% examples, 363977 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:14,924 : INFO : EPOCH 4 - PROGRESS: at 20.86% examples, 368893 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:15,970 : INFO : EPOCH 4 - PROGRESS: at 23.89% examples, 368933 words/s, in_qsize 8, out_qsize 1\n",
            "2020-05-24 15:35:16,980 : INFO : EPOCH 4 - PROGRESS: at 26.99% examples, 371196 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:17,982 : INFO : EPOCH 4 - PROGRESS: at 29.84% examples, 370499 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:18,984 : INFO : EPOCH 4 - PROGRESS: at 32.89% examples, 371110 words/s, in_qsize 8, out_qsize 0\n",
            "2020-05-24 15:35:20,018 : INFO : EPOCH 4 - PROGRESS: at 35.98% examples, 372032 words/s, in_qsize 6, out_qsize 3\n",
            "2020-05-24 15:35:21,020 : INFO : EPOCH 4 - PROGRESS: at 39.04% examples, 373459 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:22,024 : INFO : EPOCH 4 - PROGRESS: at 42.10% examples, 374734 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:23,041 : INFO : EPOCH 4 - PROGRESS: at 45.03% examples, 374159 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:24,073 : INFO : EPOCH 4 - PROGRESS: at 48.08% examples, 374552 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:25,077 : INFO : EPOCH 4 - PROGRESS: at 50.97% examples, 374360 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:26,081 : INFO : EPOCH 4 - PROGRESS: at 54.06% examples, 375293 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:27,092 : INFO : EPOCH 4 - PROGRESS: at 56.95% examples, 374999 words/s, in_qsize 8, out_qsize 2\n",
            "2020-05-24 15:35:28,108 : INFO : EPOCH 4 - PROGRESS: at 59.90% examples, 375142 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:29,124 : INFO : EPOCH 4 - PROGRESS: at 62.90% examples, 375080 words/s, in_qsize 6, out_qsize 3\n",
            "2020-05-24 15:35:30,134 : INFO : EPOCH 4 - PROGRESS: at 65.97% examples, 375685 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:31,147 : INFO : EPOCH 4 - PROGRESS: at 68.94% examples, 375627 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:35:32,157 : INFO : EPOCH 4 - PROGRESS: at 71.94% examples, 375907 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:33,197 : INFO : EPOCH 4 - PROGRESS: at 74.97% examples, 375743 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:34,197 : INFO : EPOCH 4 - PROGRESS: at 77.95% examples, 375876 words/s, in_qsize 7, out_qsize 1\n",
            "2020-05-24 15:35:35,220 : INFO : EPOCH 4 - PROGRESS: at 80.93% examples, 375670 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:36,223 : INFO : EPOCH 4 - PROGRESS: at 83.85% examples, 375494 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:37,227 : INFO : EPOCH 4 - PROGRESS: at 86.87% examples, 375812 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:38,255 : INFO : EPOCH 4 - PROGRESS: at 89.86% examples, 375798 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:39,257 : INFO : EPOCH 4 - PROGRESS: at 92.79% examples, 375647 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:40,282 : INFO : EPOCH 4 - PROGRESS: at 95.81% examples, 375462 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:41,283 : INFO : EPOCH 4 - PROGRESS: at 98.71% examples, 375581 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:41,643 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-24 15:35:41,649 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-24 15:35:41,673 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-24 15:35:41,677 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-24 15:35:41,678 : INFO : EPOCH - 4 : training on 17798082 raw words (12750898 effective words) took 33.9s, 376037 effective words/s\n",
            "2020-05-24 15:35:42,713 : INFO : EPOCH 5 - PROGRESS: at 2.85% examples, 356446 words/s, in_qsize 7, out_qsize 2\n",
            "2020-05-24 15:35:43,721 : INFO : EPOCH 5 - PROGRESS: at 5.82% examples, 365766 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:44,730 : INFO : EPOCH 5 - PROGRESS: at 8.84% examples, 369042 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:45,752 : INFO : EPOCH 5 - PROGRESS: at 11.91% examples, 371218 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:46,758 : INFO : EPOCH 5 - PROGRESS: at 14.91% examples, 372458 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:47,783 : INFO : EPOCH 5 - PROGRESS: at 17.98% examples, 373099 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:48,785 : INFO : EPOCH 5 - PROGRESS: at 20.98% examples, 373829 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:49,796 : INFO : EPOCH 5 - PROGRESS: at 23.83% examples, 372279 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:50,828 : INFO : EPOCH 5 - PROGRESS: at 26.88% examples, 372450 words/s, in_qsize 5, out_qsize 2\n",
            "2020-05-24 15:35:51,848 : INFO : EPOCH 5 - PROGRESS: at 30.01% examples, 374403 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:52,849 : INFO : EPOCH 5 - PROGRESS: at 33.06% examples, 374770 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:53,904 : INFO : EPOCH 5 - PROGRESS: at 36.20% examples, 375152 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:54,956 : INFO : EPOCH 5 - PROGRESS: at 39.33% examples, 375597 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:55,972 : INFO : EPOCH 5 - PROGRESS: at 42.27% examples, 375440 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:56,983 : INFO : EPOCH 5 - PROGRESS: at 45.25% examples, 375355 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:57,993 : INFO : EPOCH 5 - PROGRESS: at 48.19% examples, 375334 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:35:59,027 : INFO : EPOCH 5 - PROGRESS: at 51.16% examples, 374862 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:36:00,039 : INFO : EPOCH 5 - PROGRESS: at 54.12% examples, 374838 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:36:01,074 : INFO : EPOCH 5 - PROGRESS: at 56.95% examples, 373616 words/s, in_qsize 4, out_qsize 3\n",
            "2020-05-24 15:36:02,077 : INFO : EPOCH 5 - PROGRESS: at 59.91% examples, 374191 words/s, in_qsize 7, out_qsize 1\n",
            "2020-05-24 15:36:03,131 : INFO : EPOCH 5 - PROGRESS: at 63.07% examples, 374453 words/s, in_qsize 8, out_qsize 3\n",
            "2020-05-24 15:36:04,142 : INFO : EPOCH 5 - PROGRESS: at 66.13% examples, 375118 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:36:05,203 : INFO : EPOCH 5 - PROGRESS: at 69.15% examples, 374637 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:36:06,229 : INFO : EPOCH 5 - PROGRESS: at 72.22% examples, 375010 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:36:07,266 : INFO : EPOCH 5 - PROGRESS: at 75.25% examples, 374902 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:36:08,271 : INFO : EPOCH 5 - PROGRESS: at 78.32% examples, 375534 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:36:09,308 : INFO : EPOCH 5 - PROGRESS: at 81.32% examples, 375163 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:36:10,338 : INFO : EPOCH 5 - PROGRESS: at 84.29% examples, 374909 words/s, in_qsize 8, out_qsize 0\n",
            "2020-05-24 15:36:11,342 : INFO : EPOCH 5 - PROGRESS: at 87.20% examples, 374754 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:36:12,361 : INFO : EPOCH 5 - PROGRESS: at 90.13% examples, 374670 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:36:13,374 : INFO : EPOCH 5 - PROGRESS: at 93.12% examples, 374662 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-24 15:36:14,381 : INFO : EPOCH 5 - PROGRESS: at 96.13% examples, 374720 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:36:15,449 : INFO : EPOCH 5 - PROGRESS: at 99.16% examples, 374514 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-24 15:36:15,666 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-24 15:36:15,671 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-24 15:36:15,683 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-24 15:36:15,695 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-24 15:36:15,696 : INFO : EPOCH - 5 : training on 17798082 raw words (12750750 effective words) took 34.0s, 374940 effective words/s\n",
            "2020-05-24 15:36:15,697 : INFO : training on a 88990410 raw words (63751356 effective words) took 168.2s, 378949 effective words/s\n",
            "2020-05-24 15:36:15,699 : INFO : precomputing L2-norms of word weight vectors\n",
            "2020-05-24 15:36:15,835 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
            "2020-05-24 15:36:15,836 : INFO : not storing attribute vectors_norm\n",
            "2020-05-24 15:36:15,840 : INFO : not storing attribute cum_table\n",
            "2020-05-24 15:36:16,395 : INFO : saved 300features_40minwords_10context\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp3PEFwIUN5v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "addcaf84-337a-4173-d5d2-eb12280911b1"
      },
      "source": [
        "model.doesnt_match(\"man woman child kitchen\".split())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'kitchen'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkCzAhVjh2BM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}